# Story: v0.1.0 词法分析器 (Lexer)

> 实现完整的 Solidity 词法分析器，将源代码转换为 token 流

## 目标

创建一个高性能、准确的词法分析器，能够正确处理所有 Solidity 语法元素，包括：
- 所有关键字和保留字
- 所有运算符和分隔符
- 数字、字符串和十六进制字面量
- 注释（包括 NatSpec 文档注释）
- 源位置跟踪用于错误报告

## 验收标准

### 1. Token 类型定义 (token.zig)

- [ ] 定义 `Token` 结构体
  ```zig
  pub const Token = struct {
      tag: Tag,
      loc: Loc,
  };
  ```
- [ ] 定义 `Tag` 枚举 (~100+ token 类型)
- [ ] 定义 `Loc` 源位置结构
- [ ] 实现 token 字符串化方法
- [ ] 单元测试：token 创建和比较

### 2. 关键字识别

- [ ] 声明关键字: `contract`, `interface`, `library`, `function`, `modifier`, `event`, `error`, `struct`, `enum`, `constructor`, `fallback`, `receive`
- [ ] 类型关键字: `uint8`-`uint256`, `int8`-`int256`, `bool`, `address`, `bytes1`-`bytes32`, `bytes`, `string`, `mapping`
- [ ] 存储位置: `storage`, `memory`, `calldata`
- [ ] 可见性: `public`, `private`, `internal`, `external`
- [ ] 状态可变性: `pure`, `view`, `payable`, `nonpayable`
- [ ] 控制流: `if`, `else`, `for`, `while`, `do`, `break`, `continue`, `return`, `try`, `catch`
- [ ] 其他: `import`, `pragma`, `using`, `is`, `as`, `new`, `delete`, `emit`, `revert`, `require`, `assert`
- [ ] 字面量: `true`, `false`, `wei`, `gwei`, `ether`, `seconds`, `minutes`, `hours`, `days`, `weeks`, `years`
- [ ] 单元测试：每个关键字正确识别

### 3. 运算符和分隔符

- [ ] 算术运算符: `+`, `-`, `*`, `/`, `%`, `**`
- [ ] 比较运算符: `==`, `!=`, `<`, `>`, `<=`, `>=`
- [ ] 逻辑运算符: `&&`, `||`, `!`
- [ ] 位运算符: `&`, `|`, `^`, `~`, `<<`, `>>`, `>>>`
- [ ] 赋值运算符: `=`, `+=`, `-=`, `*=`, `/=`, `%=`, `|=`, `&=`, `^=`, `<<=`, `>>=`
- [ ] 增减运算符: `++`, `--`
- [ ] 三元运算符: `?`, `:`
- [ ] 成员访问: `.`
- [ ] 分隔符: `(`, `)`, `[`, `]`, `{`, `}`, `;`, `,`
- [ ] 箭头: `=>`, `->`
- [ ] 单元测试：每个运算符正确识别

### 4. 字面量

- [ ] 十进制数字: `123`, `0`, `1_000_000`
- [ ] 十六进制数字: `0x1234abcd`, `0xDEAD_BEEF`
- [ ] 科学计数法: `1e18`, `2.5e10`
- [ ] 字符串字面量: `"hello"`, `'world'`
- [ ] 十六进制字符串: `hex"deadbeef"`, `hex'1234'`
- [ ] Unicode 字符串: `unicode"你好"`
- [ ] 转义序列: `\n`, `\t`, `\\`, `\"`, `\'`, `\xNN`, `\uNNNN`
- [ ] 地址字面量: `0x1234567890123456789012345678901234567890`
- [ ] 单元测试：各种字面量格式

### 5. 注释处理

- [ ] 单行注释: `// comment`
- [ ] 多行注释: `/* comment */`
- [ ] NatSpec 单行: `/// @param x The value`
- [ ] NatSpec 多行: `/** @notice Description */`
- [ ] 嵌套注释检测（不支持，应报错）
- [ ] 未终止注释错误
- [ ] 单元测试：注释正确跳过或提取

### 6. 源位置跟踪 (source.zig)

- [ ] `Source` 结构体：管理源代码
- [ ] 行号/列号计算
- [ ] 源码片段提取
- [ ] 位置范围合并
- [ ] 单元测试：位置计算准确

### 7. 分词器实现 (tokenizer.zig)

- [ ] `Tokenizer` 结构体
  ```zig
  pub const Tokenizer = struct {
      buffer: [:0]const u8,
      index: usize,
      
      pub fn next(self: *Tokenizer) Token { ... }
  };
  ```
- [ ] 状态机实现
- [ ] 前瞻 (lookahead) 支持
- [ ] 空白字符跳过
- [ ] 行继续符处理
- [ ] Unicode 字符检测（安全：双向覆盖检测）
- [ ] 单元测试：完整分词流程

### 8. 错误处理

- [ ] `LexerError` 枚举
  - `IllegalToken`
  - `IllegalHexString`
  - `IllegalEscapeSequence`
  - `UnterminatedString`
  - `UnterminatedComment`
  - `InvalidUnicodeCodepoint`
  - `DirectionalOverride` (安全检测)
- [ ] 错误位置信息
- [ ] 错误恢复（跳过到下一个有效 token）
- [ ] 单元测试：各种错误情况

### 9. 模块集成

- [ ] `src/lexer/mod.zig` 模块入口
- [ ] 从 `src/root.zig` 导出
- [ ] 公共 API 文档注释
- [ ] `docs/lexer/README.md` 文档

## 技术设计

### Token 结构

```zig
pub const Token = struct {
    tag: Tag,
    loc: Loc,
    
    pub const Tag = enum(u8) {
        // 特殊
        invalid,
        eof,
        
        // 字面量
        identifier,
        string_literal,
        hex_string_literal,
        unicode_string_literal,
        number_literal,
        
        // 关键字 (按字母顺序)
        keyword_abstract,
        keyword_address,
        keyword_anonymous,
        // ... 约 80 个关键字
        
        // 运算符
        plus,           // +
        minus,          // -
        star,           // *
        slash,          // /
        // ... 约 40 个运算符/分隔符
        
        // 注释 (可选保留)
        comment,
        doc_comment,
        
        pub fn lexeme(self: Tag) ?[]const u8 {
            return switch (self) {
                .plus => "+",
                .keyword_contract => "contract",
                // ...
                else => null,
            };
        }
    };
    
    pub const Loc = struct {
        start: u32,
        end: u32,
        
        pub fn slice(self: Loc, source: []const u8) []const u8 {
            return source[self.start..self.end];
        }
    };
};
```

### 分词器实现模式

```zig
pub const Tokenizer = struct {
    buffer: [:0]const u8,
    index: usize,
    
    const State = enum {
        start,
        identifier,
        number,
        string,
        // ...
    };
    
    pub fn init(source: [:0]const u8) Tokenizer {
        return .{
            .buffer = source,
            .index = 0,
        };
    }
    
    pub fn next(self: *Tokenizer) Token {
        var result: Token = .{
            .tag = .eof,
            .loc = .{ .start = self.index, .end = undefined },
        };
        
        var state: State = .start;
        
        while (true) : (self.index += 1) {
            const c = self.buffer[self.index];
            switch (state) {
                .start => switch (c) {
                    0 => {
                        result.loc.end = self.index;
                        return result; // EOF
                    },
                    ' ', '\t', '\n', '\r' => {
                        result.loc.start = self.index + 1;
                    },
                    'a'...'z', 'A'...'Z', '_', '$' => {
                        state = .identifier;
                    },
                    '0'...'9' => {
                        state = .number;
                    },
                    '"', '\'' => {
                        state = .string;
                    },
                    '+' => {
                        self.index += 1;
                        if (self.buffer[self.index] == '+') {
                            result.tag = .plus_plus;
                            self.index += 1;
                        } else if (self.buffer[self.index] == '=') {
                            result.tag = .plus_equal;
                            self.index += 1;
                        } else {
                            result.tag = .plus;
                        }
                        result.loc.end = self.index;
                        return result;
                    },
                    // ... 更多字符处理
                },
                .identifier => switch (c) {
                    'a'...'z', 'A'...'Z', '0'...'9', '_', '$' => {},
                    else => {
                        result.loc.end = self.index;
                        result.tag = getKeyword(
                            self.buffer[result.loc.start..result.loc.end]
                        ) orelse .identifier;
                        return result;
                    },
                },
                // ... 更多状态
            }
        }
    }
    
    fn getKeyword(bytes: []const u8) ?Token.Tag {
        const keywords = std.ComptimeStringMap(Token.Tag, .{
            .{ "contract", .keyword_contract },
            .{ "function", .keyword_function },
            .{ "if", .keyword_if },
            // ... 所有关键字
        });
        return keywords.get(bytes);
    }
};
```

## 测试计划

### 单元测试

```zig
test "tokenize simple contract" {
    const source = 
        \\contract Foo {
        \\    uint256 value;
        \\}
    ;
    var tokenizer = Tokenizer.init(source);
    
    try expectToken(tokenizer.next(), .keyword_contract, "contract");
    try expectToken(tokenizer.next(), .identifier, "Foo");
    try expectToken(tokenizer.next(), .l_brace, "{");
    try expectToken(tokenizer.next(), .keyword_uint256, "uint256");
    try expectToken(tokenizer.next(), .identifier, "value");
    try expectToken(tokenizer.next(), .semicolon, ";");
    try expectToken(tokenizer.next(), .r_brace, "}");
    try expectToken(tokenizer.next(), .eof, "");
}

test "tokenize number literals" {
    // 测试各种数字格式
}

test "tokenize string literals" {
    // 测试各种字符串格式
}

test "tokenize operators" {
    // 测试所有运算符
}

test "handle invalid input" {
    // 测试错误处理
}
```

### 集成测试

- 使用真实 Solidity 合约文件测试
- 与官方 solc 输出对比

## 使用示例

```zig
const std = @import("std");
const lexer = @import("solidity_zig").lexer;

pub fn main() !void {
    const source = 
        \\// SPDX-License-Identifier: MIT
        \\pragma solidity ^0.8.0;
        \\
        \\contract Example {
        \\    uint256 public value;
        \\    
        \\    function setValue(uint256 _value) public {
        \\        value = _value;
        \\    }
        \\}
    ;
    
    var tokenizer = lexer.Tokenizer.init(source);
    
    while (true) {
        const token = tokenizer.next();
        if (token.tag == .eof) break;
        std.debug.print("{s}: '{s}'\n", .{
            @tagName(token.tag),
            token.loc.slice(source),
        });
    }
}
```

## 文件清单

```
src/lexer/
├── mod.zig           # 模块入口，导出公共 API
├── tokenizer.zig     # 分词器实现
├── token.zig         # Token 类型定义
└── source.zig        # 源位置管理

docs/lexer/
└── README.md         # 词法分析器文档
```

## 依赖

- `std` (Zig 标准库)
- 无外部依赖

## 完成状态

- 开始日期: 
- 完成日期: 
- 状态: ⏳ 待开始

## 参考

- [Solidity 语法](https://docs.soliditylang.org/en/latest/grammar.html)
- [liblangutil/Scanner.h](https://github.com/ethereum/solidity/blob/develop/liblangutil/Scanner.h)
- [liblangutil/Token.h](https://github.com/ethereum/solidity/blob/develop/liblangutil/Token.h)
- [Zig Tokenizer](https://github.com/ziglang/zig/blob/master/lib/std/zig/tokenizer.zig)
